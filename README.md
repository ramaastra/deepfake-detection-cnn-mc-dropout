# Evaluation on Monte Carlo Dropout Application in Deepfake Detection CNN Models

This repository contains the code of conducted experiments on the application of Monte Carlo Dropout as a regularization technique for CNN models. Specifically, the models are trained to detect deepfake contents generated by swapping two faces. A more detailed explanation of the methods and results can be found in the forthcoming paper.

## Experimental Environment

All of the experiment procedures are done using both local environment and Kaggle Notebook's environment. The local environment is utilized to prepare the dataset, including sampling, splitting, and face extraction. The Kaggle Notebook is utilized with the provided Nvidia Tesla P100 for higher computational processes, including model development and model evaluation.

Aside from the environment, there are several libraries mainly used in the experiments, such as:
- Python
- TensorFlow/Keras
- Keras Tuner
- Scikit-Learn
- OpenCV
- Matplotlib
- Pandas
- NumPy

## Datasets

To evaluate the CNN models and Monte Carlo Dropout application in a real case, three widely used deepfake detection datasets are used to perform cross-dataset evaluation. The three datasets are:
- [Celeb-DF v2](https://github.com/yuezunli/celeb-deepfakeforensics)
- [DeeperForensics-1.0](https://github.com/EndlessSora/DeeperForensics-1.0)
- [Deepfake Detection Challenge (DFDC)](https://www.kaggle.com/c/deepfake-detection-challenge)

## Results

Based on the results obtained in the cross-dataset evaluation, Monte Carlo Dropout provides a slight improvement that gives 0.5-5.5% improvement in the AUC scores of pre-trained models. However, this technique has a huge trade-off that it requires 50 times more inference time to perform 50 forward passes, leading to high computational cost.

## Citation

(Coming soon)
